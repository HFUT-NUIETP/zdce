{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ BEFORE USING\n",
    "\n",
    "### file structure\n",
    "\n",
    "```\n",
    "├─data\n",
    "│  ├─test_data\n",
    "│  │  ├─DICM\n",
    "│  │  └─LIME\n",
    "│  └─train_data\n",
    "└─snapshots\n",
    "```\n",
    "\n",
    "### experimental facility\n",
    "\n",
    "- if gpu avaliable, please uncomment \"for gpu users\" statement and comment \"for cpu users\" statement\n",
    "- if not, no need to modify anything\n",
    "\n",
    "### python environment setting\n",
    "\n",
    "- python 3.7\n",
    "- torch - pip install torch\n",
    "- torchvision - pip install torchvision\n",
    "\n",
    "### experimental parameters setting, tips is highlighted by code comment\n",
    "\n",
    "- paths\n",
    "    - training data path - default is 'L:\\\\teng\\\\Documents\\\\zdce\\\\data\\\\train_data\\\\', edit in the 2nd cell in Sec.data_loader\n",
    "- optimizer\n",
    "    - weight decay - default is 0.0001, edit in the 3rd cell in Sec.data_loader\n",
    "    - learning rate - default is 0.0001, edit in the 1st cell in Sec.training_function\n",
    "    - grad_clip_norm - default is 0.1, edit in the first cell in Sec.training_function\n",
    "- training\n",
    "    - epoches - default is 200, edit in the first cell in Sec.training_function\n",
    "    - batch_size - default is 1, edit in the first cell in Sec.training_function\n",
    "    - num_workers(mul-processings) - default is 0, edit in the first cell in Sec.training_function\n",
    "    - display training iters - default is 50, edit in the first cell in Sec.training_function\n",
    "    - save checkpoint iters - default is 100, edit in the first cell in Sec.training_function\n",
    "    - checkpoint save folder pth - default is \"snapshots/\", edit in the first cell in Sec.training_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import vgg16\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "random.seed(1143)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L_color, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x ):\n",
    "        b,c,h,w = x.shape\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Drg = torch.pow(mr-mg,2)\n",
    "        Drb = torch.pow(mr-mb,2)\n",
    "        Dgb = torch.pow(mb-mg,2)\n",
    "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
    "        \n",
    "        return k\n",
    "\n",
    "\n",
    "class L_spa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L_spa, self).__init__()\n",
    "#         # for gpu users\n",
    "#         kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "#         kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "#         kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "#         kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        # for cpu users\n",
    "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
    "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
    "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
    "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
    "        self.pool = nn.AvgPool2d(4)\n",
    "\n",
    "\n",
    "    def forward(self, org , enhance ):\n",
    "        b,c,h,w = org.shape\n",
    "        org_mean = torch.mean(org,1,keepdim=True)\n",
    "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
    "        org_pool =  self.pool(org_mean)\n",
    "        enhance_pool = self.pool(enhance_mean)\n",
    "#         # for gpu users\n",
    "#         weight_diff =torch.max(torch.FloatTensor([1]).cuda() + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).cuda(),torch.FloatTensor([0]).cuda()),torch.FloatTensor([0.5]).cuda())\n",
    "#         E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).cuda()) ,enhance_pool-org_pool)\n",
    "        # for cpu users\n",
    "        weight_diff =torch.max(torch.FloatTensor([1]) + 10000*torch.min(org_pool - torch.FloatTensor([0.3]),torch.FloatTensor([0])),torch.FloatTensor([0.5]))\n",
    "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5])) ,enhance_pool-org_pool)\n",
    "        \n",
    "        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n",
    "        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n",
    "        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n",
    "        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n",
    "        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n",
    "        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n",
    "        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n",
    "        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n",
    "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
    "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
    "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
    "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
    "        E = (D_left + D_right + D_up +D_down)\n",
    "\n",
    "        return E\n",
    "\n",
    "\n",
    "class L_exp(nn.Module):\n",
    "    def __init__(self,patch_size,mean_val):\n",
    "        super(L_exp, self).__init__()\n",
    "        self.pool = nn.AvgPool2d(patch_size)\n",
    "        self.mean_val = mean_val\n",
    "        \n",
    "\n",
    "    def forward(self, x ):\n",
    "        b,c,h,w = x.shape\n",
    "        x = torch.mean(x,1,keepdim=True)\n",
    "        mean = self.pool(x)\n",
    "#         # for gpu users\n",
    "#         d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).cuda(),2))\n",
    "        # for cpu users\n",
    "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ),2))\n",
    "\n",
    "        return d\n",
    "        \n",
    "\n",
    "class L_TV(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=1):\n",
    "        super(L_TV,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
    "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        \n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "    \n",
    "\n",
    "class Sa_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sa_Loss, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x ):\n",
    "        b,c,h,w = x.shape\n",
    "        r,g,b = torch.split(x , 1, dim=1)\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Dr = r-mr\n",
    "        Dg = g-mg\n",
    "        Db = b-mb\n",
    "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
    "        k = torch.mean(k)\n",
    "        \n",
    "        return k\n",
    "\n",
    "\n",
    "class perception_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(perception_loss, self).__init__()\n",
    "        features = vgg16(pretrained=True).features\n",
    "        self.to_relu_1_2 = nn.Sequential() \n",
    "        self.to_relu_2_2 = nn.Sequential() \n",
    "        self.to_relu_3_3 = nn.Sequential()\n",
    "        self.to_relu_4_3 = nn.Sequential()\n",
    "        for x in range(4):\n",
    "            self.to_relu_1_2.add_module(str(x), features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.to_relu_2_2.add_module(str(x), features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.to_relu_3_3.add_module(str(x), features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.to_relu_4_3.add_module(str(x), features[x])\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.to_relu_1_2(x)\n",
    "        h_relu_1_2 = h\n",
    "        h = self.to_relu_2_2(h)\n",
    "        h_relu_2_2 = h\n",
    "        h = self.to_relu_3_3(h)\n",
    "        h_relu_3_3 = h\n",
    "        h = self.to_relu_4_3(h)\n",
    "        h_relu_4_3 = h\n",
    "\n",
    "        return h_relu_4_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enhance_net_nopool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(enhance_net_nopool, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        number_f = 32\n",
    "        self.e_conv1 = nn.Conv2d(3,number_f,3,1,1,bias=True) \n",
    "        self.e_conv2 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "        self.e_conv3 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "        self.e_conv4 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "        self.e_conv5 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "        self.e_conv6 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "        self.e_conv7 = nn.Conv2d(number_f*2,24,3,1,1,bias=True) \n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.e_conv1(x))\n",
    "        x2 = self.relu(self.e_conv2(x1))\n",
    "        x3 = self.relu(self.e_conv3(x2))\n",
    "        x4 = self.relu(self.e_conv4(x3))\n",
    "        x5 = self.relu(self.e_conv5(torch.cat([x3,x4],1)))\n",
    "        x6 = self.relu(self.e_conv6(torch.cat([x2,x5],1)))\n",
    "        x_r = F.tanh(self.e_conv7(torch.cat([x1,x6],1)))\n",
    "        r1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, 3, dim=1)\n",
    "        x = x + r1*(torch.pow(x,2)-x)\n",
    "        x = x + r2*(torch.pow(x,2)-x)\n",
    "        x = x + r3*(torch.pow(x,2)-x)\n",
    "        enhance_image_1 = x + r4*(torch.pow(x,2)-x)\n",
    "        x = enhance_image_1 + r5*(torch.pow(enhance_image_1,2)-enhance_image_1)\n",
    "        x = x + r6*(torch.pow(x,2)-x)\n",
    "        x = x + r7*(torch.pow(x,2)-x)\n",
    "        enhance_image = x + r8*(torch.pow(x,2)-x)\n",
    "        r = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],1)\n",
    "\n",
    "        return enhance_image_1,enhance_image,r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_train_list(lowlight_images_path):\n",
    "    image_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n",
    "    train_list = image_list_lowlight\n",
    "    random.shuffle(train_list)\n",
    "\n",
    "    return train_list\n",
    "\n",
    "\n",
    "class lowlight_loader(data.Dataset):\n",
    "    def __init__(self, lowlight_images_path):\n",
    "        self.train_list = populate_train_list(lowlight_images_path) \n",
    "        self.size = 256\n",
    "        self.data_list = self.train_list\n",
    "        print(\"Total training examples:\", len(self.train_list))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_lowlight_path = self.data_list[index]\n",
    "        data_lowlight = Image.open(data_lowlight_path)\n",
    "        data_lowlight = data_lowlight.resize((self.size,self.size), Image.ANTIALIAS)\n",
    "        data_lowlight = (np.asarray(data_lowlight)/255.0) \n",
    "        data_lowlight = torch.from_numpy(data_lowlight).float()\n",
    "\n",
    "        return data_lowlight.permute(2,0,1)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------param edit start----------\n",
    "# training dataset path\n",
    "# ----------param edit end----------\n",
    "train_data_pth = 'L:\\\\teng\\\\Documents\\\\zdce\\\\data\\\\train_data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "\n",
    "def train(load_from_pretrained=False):\n",
    "#     # for gpu users\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "#     DCE_net = model.enhance_net_nopool().cuda()\n",
    "    # for cpu users\n",
    "    DCE_net = enhance_net_nopool()\n",
    "    \n",
    "    DCE_net.apply(weights_init)\n",
    "    if load_from_pretrained == True:\n",
    "        DCE_net.load_state_dict(torch.load(config.pretrain_dir))\n",
    "    # training data path\n",
    "    train_dataset = lowlight_loader(train_data_pth)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# ----------param edit start----------\n",
    "    # edit batch_size, num_workers\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "# ----------param edit end----------\n",
    "    LL_color = L_color()\n",
    "    LL_spa = L_spa()\n",
    "    LL_exp = L_exp(16,0.6)\n",
    "    LL_TV = L_TV()\n",
    "    \n",
    "# ----------param edit start----------\n",
    "    # setting learning rate,weight_decay\n",
    "    optimizer = torch.optim.Adam(DCE_net.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "# ----------param edit end----------\n",
    "    \n",
    "    DCE_net.train()\n",
    "# ----------param edit start----------\n",
    "    # setting training epoches    \n",
    "    for epoch in range(200):\n",
    "# ----------param edit end----------\n",
    "        for iteration, img_lowlight in enumerate(train_loader):\n",
    "\n",
    "#             # for gpu users\n",
    "#             img_lowlight = img_lowlight.cuda()\n",
    "            # for cpu users\n",
    "            img_lowlight = img_lowlight\n",
    "            enhanced_image_1,enhanced_image,A  = DCE_net(img_lowlight)\n",
    "            Loss_TV = 200*LL_TV(A)\n",
    "            loss_spa = torch.mean(LL_spa(enhanced_image, img_lowlight))\n",
    "            loss_col = 5*torch.mean(LL_color(enhanced_image))\n",
    "            loss_exp = 10*torch.mean(LL_exp(enhanced_image))\n",
    "            loss =  Loss_TV + loss_spa + loss_col + loss_exp\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "# ----------param edit start----------\n",
    "            # edit grad_clip_norm\n",
    "            torch.nn.utils.clip_grad_norm(DCE_net.parameters(),0.1)\n",
    "# ----------param edit end----------\n",
    "            optimizer.step()\n",
    "\n",
    "# ----------param edit start----------\n",
    "            # edit diskplay_training_iters\n",
    "            if ((iteration+1) % 50) == 0:\n",
    "# ----------param edit end----------\n",
    "                print(\"Loss at iteration\", iteration+1, \":\", loss.item())\n",
    "# ----------param edit start----------\n",
    "            # edit save_checkpoint_iters\n",
    "            if ((iteration+1) % 100) == 0:\n",
    "# ----------param edit end----------\n",
    "# ----------param edit start----------\n",
    "                # checkpoint save folder pth\n",
    "                torch.save(DCE_net.state_dict(), \"snapshots/\" + \"Epoch\" + str(epoch) + '.pth')\n",
    "# ----------param edit end----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teng\\miniconda3\\envs\\zdce\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
